# Ambient Music: Perception Science & Web Implementation

---

## Part I: The Neuroscience of Ambient Music Perception

### Susan Rogers' Seven Dimensions of Music

Dr. Susan Rogers -- former chief sound engineer for Prince, now a cognitive neuroscientist and professor at Berklee College of Music -- presents a framework in her 2022 book *This Is What It Sounds Like: What the Music You Love Says About You* (co-authored with Ogi Ogas) that every listener has a unique "listener profile" shaped by their brain's response to seven key dimensions of any piece of music.

These seven dimensions split into two categories:

**Aesthetic Dimensions** (subjective judgments about the music's character):
1. **Authenticity** -- the subjective impression that the emotion expressed in a musical performance is genuine and uncontrived
2. **Realism** -- the opposite of abstraction; whether music sounds like it was made by real humans on real instruments or exists in a computer-generated, virtual sonic space
3. **Novelty** -- perception of the music's originality, from the comfortably familiar to the boundary-pushing and experimental

**Musical Dimensions** (responses to the music's structural elements):
4. **Melody** -- "the heart of a record"; varies on axes of range (wide vs. narrow), articulation (legato vs. staccato), and complexity (simple vs. ornate)
5. **Lyrics** -- "the mind of a record"; ideas expressed through words, from plainly spoken truths to abstract poetry
6. **Rhythm** -- "the hips of a record"; the physical compulsion to move, from gentle pulse to irresistible groove
7. **Timbre** -- "the face of a record"; the unique sonic portrait that distinguishes instruments and voices, establishing genre identity

Each listener possesses a personal "sweet spot" on each dimension -- a Goldilocks zone where the music gives maximum pleasure. Rogers' central insight is that *everyone's taste is valid*, and understanding your own profile is an act of self-discovery.

Sources: [Next Big Idea Club - Bookbite](https://nextbigideaclub.com/magazine/sounds-like-music-love-says-bookbite/37223/), [The Constant Reader - Book Review](https://musicman6724.wordpress.com/2023/09/29/this-is-what-it-sounds-like-by-susan-rogers-and-ogi-ogas/), [Music & Hi-fi Appreciations](https://hifiauditions.wordpress.com/2025/01/07/princes-engineer-explains-scientifically-why-you-love-your-favorite-songs/)

---

### How Ambient Music Maps to the Seven Dimensions

Ambient music occupies a distinctive and revealing position within Rogers' framework. While the book does not dedicate a chapter exclusively to ambient music, applying the dimensional model to the genre illuminates *why* ambient appeals to certain listener profiles -- and why it puzzles or bores others.

#### Realism Dimension: Deep in Abstraction

Rogers defines the realism spectrum as running from *realistic* (music that sounds like real humans playing real instruments in a real space, where you can "picture the musicians in your mind's eye") to *abstract* (music created on "virtual instruments that exist only in computer code," with digitally constructed sounds that have no acoustic counterpart).

Rogers explicitly notes that "most techno and electronic dance music is abstract." Ambient music sits even further toward the abstract pole than EDM. Classic ambient works by Brian Eno, Aphex Twin (*Selected Ambient Works Volume II*), or Stars of the Lid employ synthesizers, processed field recordings, and digital effects to create sonic worlds that bear no resemblance to a band playing in a room. Even when ambient uses acoustic sources (piano in Eno's *Music for Airports*, guitar in *Apollo*), the processing -- extreme reverb, tape manipulation, time-stretching -- pushes the result toward abstraction.

Rogers draws an analogy: just as the invention of the camera freed painters to explore abstraction (since photographic realism was now handled by a machine), digital audio technology freed musicians to create sounds that exist nowhere in nature. Ambient music is among the purest expressions of this freedom.

**Listener implication:** People who score high on the "abstract" end of the realism dimension -- those who enjoy synthetic textures and feel no need to visualize a live performer -- are predisposed to enjoy ambient music. Those who need to picture a guitarist strumming or a drummer sweating will find ambient alienating.

Sources: [Isthmus - Three Questions for Susan Rogers](https://isthmus.com/arts/books/three-questions-for-susan-rogers/), [Steve Pick Substack Review](https://stevepick.substack.com/p/i-read-a-book-this-is-what-it-sounds), [Official Book Site - Chapter 2: Realism](https://www.thisiswhatitsoundslike.com/chapter/chapter-2-realism)

#### Novelty Dimension: The Slow Burn of the Unfamiliar

Rogers describes novelty as a spectrum from listeners who prefer "classic, familiar forms" that showcase composition and performance technique, to those who crave "musically ground-breaking or daring" work. Every listener has a personal Goldilocks zone: too familiar and you're bored, too strange and you're lost.

Ambient music has a complex relationship with novelty. On the surface, a single sustained drone or slowly evolving pad can sound uneventful -- almost anti-novel. But at a deeper level, ambient's refusal of conventional song structure (no verses, no choruses, no hooks, no resolution) is itself radically novel to most listeners raised on pop forms. The first time someone hears Eno's *Music for Airports*, the novelty is not in the notes themselves but in the *absence* of expected musical events.

The neuroscience behind this is the brain's prediction machinery. Andrew Huberman's Huberman Lab podcast discusses how listening to music activates both prediction circuits and novelty-detection circuits simultaneously. Ambient music creates a state where predictions are constantly *slightly* confounded -- not through dissonance or shock, but through temporal unpredictability. You cannot predict when the next note will arrive or what combination of tones will emerge. This creates a sustained state of mild novelty that some brains find deeply pleasurable.

**Listener implication:** High-novelty seekers may initially dismiss ambient as "not enough happening," but those who can shift their attention from macro-structure to micro-texture often discover that ambient provides an exceptionally rich novelty experience at the timbral and temporal level.

Sources: [Paul Heilker Blog - Book Review](https://paulheilker.com/blogs/blog/posts/7364620/10-march-2024-this-is-what-it-sounds-like), [Huberman Lab - How to Use Music to Boost Motivation, Mood & Improve Learning](https://www.hubermanlab.com/episode/how-to-use-music-to-boost-motivation-mood-and-improve-learning)

#### Melody Dimension: Minimal or Absent

Rogers describes melody as the emotional core of music, varying in range (wide vs. narrow), articulation (legato vs. staccato), and complexity. Ambient music frequently minimizes or entirely eliminates recognizable melody. When melodic fragments do appear (as in Eno's *Music for Airports* or Harold Budd's piano works), they tend to be:

- **Narrow in range** -- often spanning only a few notes
- **Extremely legato** -- long, sustained tones that bleed into each other
- **Simple but non-repetitive** -- fragments that appear, drift, and dissolve rather than forming memorable themes

This means listeners whose sweet spot demands strong melodic content -- those who hum along, who remember a tune after one listen, who prize "a great melody" above all else -- will find ambient unsatisfying. Ambient appeals instead to listeners whose melodic sweet spot is low, who do not require melodic hooks to engage with music.

#### Rhythm Dimension: The Absence of Pulse

Rhythm, in Rogers' framework, is "music's hips" -- the dimension that compels physical movement. Most ambient music deliberately subverts or eliminates rhythmic pulse. There is no beat to tap your foot to in *Music for Airports*, no groove in Stars of the Lid's *And Their Refinement of the Decline*.

This is perhaps ambient's most divisive trait. For listeners whose sweet spot is high on the rhythm dimension -- those who need a beat to feel engaged, who experience music primarily through bodily movement -- ambient registers as essentially *inert*. For listeners whose rhythm sweet spot is low, the absence of pulse is liberating: it removes the metronomic grid and allows attention to float freely through sonic space.

Some ambient subgenres reintroduce rhythm (ambient techno, ambient house), creating a bridge for rhythm-oriented listeners to access ambient textures.

#### Timbre Dimension: Where Ambient Lives

If there is one dimension where ambient music excels -- where it is arguably the *richest* of all genres -- it is timbre. Rogers describes timbre as "the face of a record" and notes that timbral preferences vary on axes including "brass vs. woodwind, bowed vs. plucked, rough vs. smooth, and acoustic vs. electronic."

Ambient music is fundamentally *about* timbre. Strip away melody, lyrics, and rhythm, and what remains is pure sonic texture: the grain of a reversed piano note decaying through reverb, the shimmer of detuned oscillators slowly beating against each other, the rustling density of processed field recordings layered beneath a synthetic pad. Ambient composers are, in essence, timbre sculptors.

This explains why ambient appeals so strongly to listeners who score high on the timbre dimension -- audiophiles who are drawn to "the sound itself," who notice and care about the difference between a Fender Rhodes and a Wurlitzer, between analog warmth and digital precision. These listeners experience ambient not as "background noise" but as an endlessly detailed tapestry of sonic color.

**Key insight:** Rogers' framework reveals that ambient music is not "lacking" in musical content -- rather, it concentrates its content almost entirely in the timbre dimension while deliberately minimizing melody, lyrics, and rhythm. It is a genre that rewards listeners whose profiles are weighted toward timbre and abstraction.

Sources: [Berklee College of Music - Susan Rogers](https://college.berklee.edu/people/susan-rogers), [Chris Coyier - Authenticity](https://chriscoyier.net/2023/03/20/authenticity/)

#### Lyrics Dimension: Silence as Statement

Ambient music is almost always instrumental. The absence of lyrics is not merely incidental but philosophical -- Eno explicitly wanted music that would not demand linguistic processing. In Rogers' framework, this means ambient music scores near zero on the lyrics dimension. Listeners who need lyrical content to engage with music ("I listen to songs for the words") will find nothing to hold onto.

Interestingly, when vocals do appear in ambient (Eno's *Music for Airports* uses wordless vocal loops, Grouper layers unintelligible singing), they function as *timbre* rather than lyrics -- the human voice used as another textural element, not as a vehicle for meaning.

#### Authenticity Dimension: A Different Kind of Genuine

Rogers notes that everyone values authenticity, but what registers as "authentic" varies enormously between listeners. Ambient poses an interesting challenge: it can feel either deeply authentic (a sincere attempt to create beauty and contemplative space) or deeply inauthentic (cold, emotionless, machine-generated). The perception depends entirely on the listener's prior exposure and cultural context.

For many ambient listeners, the genre's lack of commercial calculation -- its refusal to pander with catchy hooks or relatable lyrics -- reads as profoundly authentic. Brian Eno's stated goal of creating music "as ignorable as it is interesting" is itself an anti-commercial stance that registers as genuine artistic intent.

---

### Brian Eno and the Philosophy of Ambient Music

Brian Eno coined the term "ambient music" with the release of *Ambient 1: Music for Airports* in 1978. His foundational statement remains the most precise definition of the genre's intent:

> "Ambient Music must be able to accommodate many levels of listening attention without enforcing one in particular; it must be as ignorable as it is interesting."

This principle directly engages with what Rogers would later frame as the listener's attention allocation across dimensions. Eno was designing music that could function across multiple cognitive modes:

- **Active listening** -- attending to timbral details, emergent harmonic combinations, spatial characteristics
- **Peripheral listening** -- the music colors the environment without demanding conscious processing
- **Transitional listening** -- drifting in and out of conscious attention, the music serving as a flexible cognitive companion

#### "Musical Wallpaper" vs. Active Listening

The concept of ambient music as "musical wallpaper" was embedded in Eno's original intent. Erik Satie had coined the term *musique d'ameublement* ("furniture music") decades earlier, and Eno explicitly acknowledged this lineage. However, Eno's formulation is more nuanced than "background music" -- the key phrase is "as ignorable *as it is interesting*." The music must reward close attention even as it permits inattention.

From a neuroscience perspective, this maps onto the distinction between *focused attention* and *diffuse attention* networks in the brain. Music that is "ignorable but interesting" occupies a zone where it does not trigger the salience network (which would demand conscious attention) but does maintain sufficient complexity to engage the default mode network (associated with mind-wandering, creativity, and introspection).

Rogers' research on auditory memory and the perception of musical signals at Berklee's Music Perception & Cognition Laboratory provides a framework for understanding this dual mode: music with low salience features (no sudden dynamic changes, no lyrics demanding comprehension, no rhythmic urgency) can exist in a perceptual background while still providing ongoing neural reward through timbral complexity and harmonic subtlety.

Sources: [Designing Sound - Interview with Dr. Susan Rogers](https://designingsound.org/2016/05/26/music-cognition-and-psychoacoustic-research-an-interview-with-dr-susan-rogers/), [Open Culture - Brian Eno Explains Ambient Music Origins](https://www.openculture.com/2021/03/brian-eno-explains-the-origins-of-ambient-music.html)

#### Generative Music: The System as Composer

Eno popularized the term "generative music" in 1995 to describe music created by systems rather than composed note-by-note. His generative philosophy is rooted in the idea that the artist designs *rules* rather than *outcomes*.

> "The considerations that are important, then, become questions of how the system works and most important of all, what you feed into the system." -- Brian Eno

**Music for Airports: The Tape Loop System**

The best-documented example is *Music for Airports* (1978), particularly the track "2/1." Eno recorded seven vocal phrases onto tape loops of deliberately different lengths, then ran them simultaneously on separate tape machines in Conny Plank's studio:

| Loop   | Note  | Duration   |
|--------|-------|------------|
| Loop 1 | High Ab | 17.8 sec  |
| Loop 2 | Eb    | 16.2 sec   |
| Loop 3 | High F | 19.6 sec  |
| Loop 4 | C     | 20.1 sec   |
| Loop 5 | Low Ab | 21.3 sec  |
| Loop 6 | Low F | 24.7 sec   |
| Loop 7 | Db    | 31.8 sec   |

Because the loop lengths are *incommensurable* (their ratios are irrational, meaning they will never perfectly realign), the combinations of notes shift continuously, creating an endlessly evolving harmonic landscape from a fixed set of simple elements.

Eno described this: "I just set all of these loops running and let them configure in whichever way they wanted to." The system produces "music" in the fullest sense -- harmonic movement, tension and release, rhythmic patterning -- but none of it was composed in the traditional sense. It emerged from the interaction of simple rules (loop lengths) with simple materials (seven sustained vocal notes).

This is the foundational idea behind all generative ambient music, including the web-based implementations discussed in Part II.

Sources: [Reverb Machine - Deconstructing Music for Airports](https://reverbmachine.com/blog/deconstructing-brian-eno-music-for-airports/), [Teropa - How Generative Music Works](https://teropa.info/loop/), [Bill Milkowski - Brian Eno's Generative Genius](https://billmilkowski.substack.com/p/brian-enos-generative-genius)

#### Oblique Strategies: Constrained Randomness as Creative Tool

In 1975, Eno and artist Peter Schmidt created *Oblique Strategies*, a deck of cards with cryptic prompts ("Use an unacceptable color," "Honor thy error as a hidden intention," "What would your closest friend do?"). These cards introduce controlled randomness into the creative process -- not chaos, but *constrained indeterminacy*.

This philosophy pervades ambient generative systems: you define the constraints (scale, tempo range, timbral palette, probability weights) and let randomness operate *within* those constraints. The result is music that is always different but always coherent -- never repeating, but never wrong.

Sources: [VentureBeat - How Oblique Strategies Predicted AI](https://venturebeat.com/ai/how-brian-eno-anticipated-the-creative-dynamics-of-ai-by-decades), [Wikipedia - Generative Music](https://en.wikipedia.org/wiki/Generative_music)

#### The Reflection App (2017)

Eno and developer Peter Chilvers created *Reflection* as both a fixed album and a generative iOS app. The app runs indefinitely, using algorithms to vary and permutate musical elements. Crucially, the rules themselves change with the time of day: harmony is brighter in the morning, transitions through the afternoon to the original key by evening, and in the early hours conditions thin out the notes and slow everything down.

The app presents "an endless and endlessly changing version of the piece of music" with essentially no user controls other than a pause button. It represents the purest expression of Eno's generative philosophy: the artist designs the system, then the system produces the music.

Sources: [Generative Music - Reflection](https://www.generativemusic.com/reflection.html), [App Store - Brian Eno: Reflection](https://apps.apple.com/us/app/brian-eno-reflection/id1180524479)

---

### Listener Profiles and Ambient Music Preference

Synthesizing Rogers' framework with the characteristics of ambient music, we can sketch the listener profile most likely to gravitate toward ambient:

| Dimension    | Ambient Sweet Spot                  | Description                                        |
|--------------|-------------------------------------|----------------------------------------------------|
| Authenticity | Moderate-High                       | Values artistic sincerity over commercial appeal    |
| Realism      | Abstract                            | Comfortable with synthetic and processed sound      |
| Novelty      | Moderate-High (structural novelty)  | Enjoys unfamiliar forms, but not shock-value        |
| Melody       | Low                                 | Does not require memorable melodic hooks            |
| Lyrics       | Minimal/None                        | Does not need words to engage with music            |
| Rhythm       | Low                                 | Does not need a beat to feel engaged                |
| Timbre       | Very High                           | Deeply attuned to sonic texture and color           |

This profile represents a listener who processes music primarily through *texture and atmosphere* rather than through *song structure and groove*. Rogers' framework makes clear that this is neither a superior nor inferior way of listening -- it is simply one configuration of the seven-dimensional space of musical preference.

---

## Part II: Coding Ambient Music for the Web

### Foundations: The Web Audio API

The [Web Audio API](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API) is a high-performance audio processing system built into all modern browsers. It provides a modular audio graph architecture where nodes (sources, effects, destinations) are connected together to form signal chains.

Key concepts for ambient music generation:

- **AudioContext** -- the master container for all audio operations
- **OscillatorNode** -- generates waveforms (sine, square, sawtooth, triangle) at specified frequencies
- **GainNode** -- controls volume; used for envelopes, LFOs, and mixing
- **BiquadFilterNode** -- lowpass, highpass, bandpass, and other filter types
- **ConvolverNode** -- convolution reverb using impulse responses
- **DelayNode** -- audio delay with feedback for echo effects
- **PannerNode / StereoPannerNode** -- spatial positioning of sound sources
- **AudioBufferSourceNode** -- plays back pre-recorded audio samples

Sources: [MDN - Web Audio API](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API), [MDN - Using the Web Audio API](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Using_Web_Audio_API)

---

### 1. Generative Ambient Techniques with Raw Web Audio API

#### 1.1 Drone Generation: Layered Oscillators with Detuning

The simplest ambient technique is layering oscillators with slight frequency offsets. When two oscillators play nearly the same frequency, they produce *beating* -- slow amplitude variations that create movement and warmth.

```javascript
// Ambient Drone Generator
class AmbientDrone {
  constructor() {
    this.ctx = new AudioContext();
    this.master = this.ctx.createGain();
    this.master.gain.value = 0.3;
    this.master.connect(this.ctx.destination);
    this.oscillators = [];
  }

  createLayer(baseFreq, detuneAmount, waveform = 'sine') {
    const osc = this.ctx.createOscillator();
    const gain = this.ctx.createGain();
    const filter = this.ctx.createBiquadFilter();

    osc.type = waveform;
    osc.frequency.value = baseFreq;
    osc.detune.value = detuneAmount; // cents

    filter.type = 'lowpass';
    filter.frequency.value = 800;
    filter.Q.value = 1;

    gain.gain.value = 0.15;

    osc.connect(filter);
    filter.connect(gain);
    gain.connect(this.master);
    osc.start();

    this.oscillators.push({ osc, gain, filter });
    return { osc, gain, filter };
  }

  createDrone(rootHz) {
    // Fundamental with slight detune layers
    this.createLayer(rootHz, 0, 'sine');
    this.createLayer(rootHz, 7, 'sine');       // ~7 cents sharp
    this.createLayer(rootHz, -5, 'sine');      // ~5 cents flat
    // Octave above, softer
    this.createLayer(rootHz * 2, 3, 'sine');
    // Fifth above
    this.createLayer(rootHz * 1.5, -4, 'triangle');
    // Sub-octave
    this.createLayer(rootHz / 2, 2, 'sine');
  }

  stop() {
    this.oscillators.forEach(({ osc, gain }) => {
      gain.gain.exponentialRampToValueAtTime(0.001, this.ctx.currentTime + 3);
      osc.stop(this.ctx.currentTime + 3.1);
    });
  }
}

// Usage
const drone = new AmbientDrone();
drone.createDrone(110); // A2, a rich low drone
```

#### 1.2 Slow LFO Modulation

Low Frequency Oscillators (LFOs) below 20 Hz create slow, evolving changes in pitch, volume, and filter cutoff -- essential for the "living" quality of ambient music.

```javascript
function createLFO(ctx, targetParam, rate, depth) {
  const lfo = ctx.createOscillator();
  const lfoGain = ctx.createGain();

  lfo.type = 'sine';
  lfo.frequency.value = rate; // Hz, typically 0.01 - 0.5 for ambient

  lfoGain.gain.value = depth;

  lfo.connect(lfoGain);
  lfoGain.connect(targetParam);
  lfo.start();

  return { lfo, lfoGain };
}

// Example: Slow filter sweep on a drone
const ctx = new AudioContext();
const osc = ctx.createOscillator();
const filter = ctx.createBiquadFilter();
const gain = ctx.createGain();

osc.type = 'sawtooth';
osc.frequency.value = 55; // A1

filter.type = 'lowpass';
filter.frequency.value = 400;
filter.Q.value = 2;

gain.gain.value = 0.2;

osc.connect(filter);
filter.connect(gain);
gain.connect(ctx.destination);

// Modulate filter cutoff: 0.05 Hz = one cycle every 20 seconds
createLFO(ctx, filter.frequency, 0.05, 300);

// Modulate detune: 0.03 Hz = glacially slow pitch drift
createLFO(ctx, osc.detune, 0.03, 15);

osc.start();
```

Sources: [Dobrian - LFOs in Web Audio](https://dobrian.github.io/cmp/topics/building-a-synthesizer-with-web-audio-api/2.lfos.html), [GitHub - mmckegg/lfo](https://github.com/mmckegg/lfo)

#### 1.3 Reverb and Delay for Spatial Depth

Ambient music lives in reverb. The Web Audio API's `ConvolverNode` applies convolution reverb using impulse response recordings, while `DelayNode` creates echo effects.

```javascript
class AmbientSpace {
  constructor(ctx) {
    this.ctx = ctx;
    this.convolver = ctx.createConvolver();
    this.reverbGain = ctx.createGain();
    this.dryGain = ctx.createGain();
    this.delay = ctx.createDelay(5.0);
    this.feedback = ctx.createGain();
    this.delayFilter = ctx.createBiquadFilter();

    // Reverb mix
    this.reverbGain.gain.value = 0.7; // heavy reverb
    this.dryGain.gain.value = 0.3;

    // Delay with filtered feedback
    this.delay.delayTime.value = 1.5; // 1.5 second delay
    this.feedback.gain.value = 0.4;   // 40% feedback
    this.delayFilter.type = 'lowpass';
    this.delayFilter.frequency.value = 2000; // darken repeats

    // Wire feedback loop: delay -> filter -> feedback gain -> delay
    this.delay.connect(this.delayFilter);
    this.delayFilter.connect(this.feedback);
    this.feedback.connect(this.delay);
  }

  // Generate synthetic impulse response (no external file needed)
  async generateImpulseResponse(duration = 4, decay = 3) {
    const length = this.ctx.sampleRate * duration;
    const impulse = this.ctx.createBuffer(2, length, this.ctx.sampleRate);

    for (let channel = 0; channel < 2; channel++) {
      const data = impulse.getChannelData(channel);
      for (let i = 0; i < length; i++) {
        // Exponential decay with random noise
        data[i] = (Math.random() * 2 - 1) *
                  Math.pow(1 - i / length, decay);
      }
    }
    this.convolver.buffer = impulse;
  }

  connect(source, destination) {
    // Dry path
    source.connect(this.dryGain);
    this.dryGain.connect(destination);
    // Reverb path
    source.connect(this.convolver);
    this.convolver.connect(this.reverbGain);
    this.reverbGain.connect(destination);
    // Delay path (fed from reverb output)
    this.reverbGain.connect(this.delay);
    this.delay.connect(destination);
  }
}

// Usage
const ctx = new AudioContext();
const space = new AmbientSpace(ctx);
await space.generateImpulseResponse(5, 2.5);
// Now connect any source through it:
// space.connect(myOscillator, ctx.destination);
```

Sources: [MDN - Advanced Audio Techniques](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Advanced_techniques), [SitePoint - Creating Immersive Audio Experiences](https://www.sitepoint.com/creating-fun-immersive-audio-experiences-web-audio/)

#### 1.4 Randomized Note Selection from Scales

Ambient melody generation uses constrained randomness -- selecting notes from harmonically safe scales so that any combination sounds pleasant.

```javascript
// Scale definitions (semitones from root)
const SCALES = {
  pentatonicMajor: [0, 2, 4, 7, 9],
  pentatonicMinor: [0, 3, 5, 7, 10],
  wholeTone:       [0, 2, 4, 6, 8, 10],
  dorian:          [0, 2, 3, 5, 7, 9, 10],
  lydian:          [0, 2, 4, 6, 7, 9, 11],
  mixolydian:      [0, 2, 4, 5, 7, 9, 10],
};

function midiToFreq(midi) {
  return 440 * Math.pow(2, (midi - 69) / 12);
}

function getScaleNotes(rootMidi, scale, octaveRange = 3) {
  const notes = [];
  for (let octave = 0; octave < octaveRange; octave++) {
    for (const interval of scale) {
      notes.push(rootMidi + interval + (octave * 12));
    }
  }
  return notes;
}

function weightedRandomNote(notes, currentIndex) {
  // Favor notes close to the current note (stepwise motion)
  const weights = notes.map((_, i) => {
    const distance = Math.abs(i - currentIndex);
    return Math.exp(-distance * 0.5); // exponential falloff
  });
  const totalWeight = weights.reduce((a, b) => a + b, 0);
  let random = Math.random() * totalWeight;

  for (let i = 0; i < weights.length; i++) {
    random -= weights[i];
    if (random <= 0) return i;
  }
  return notes.length - 1;
}
```

#### 1.5 Field Recording Playback and Layering

Ambient music often layers processed field recordings (rain, wind, birds, urban hum) beneath synthetic elements.

```javascript
class FieldRecordingLayer {
  constructor(ctx) {
    this.ctx = ctx;
    this.buffers = new Map();
  }

  async loadSample(name, url) {
    const response = await fetch(url);
    const arrayBuffer = await response.arrayBuffer();
    const audioBuffer = await this.ctx.decodeAudioData(arrayBuffer);
    this.buffers.set(name, audioBuffer);
  }

  play(name, { loop = true, volume = 0.3, filterFreq = 5000, pan = 0 } = {}) {
    const buffer = this.buffers.get(name);
    if (!buffer) return;

    const source = this.ctx.createBufferSource();
    const gain = this.ctx.createGain();
    const filter = this.ctx.createBiquadFilter();
    const panner = this.ctx.createStereoPanner();

    source.buffer = buffer;
    source.loop = loop;

    gain.gain.value = volume;
    filter.type = 'lowpass';
    filter.frequency.value = filterFreq;
    panner.pan.value = pan; // -1 to 1

    // Fade in over 5 seconds
    gain.gain.setValueAtTime(0, this.ctx.currentTime);
    gain.gain.linearRampToValueAtTime(volume, this.ctx.currentTime + 5);

    source.connect(filter);
    filter.connect(gain);
    gain.connect(panner);
    panner.connect(this.ctx.destination);

    source.start();
    return { source, gain, filter, panner };
  }
}
```

---

### 2. Tone.js for Ambient Music

[Tone.js](https://tonejs.github.io/) wraps the Web Audio API in a musician-friendly framework with built-in synths, effects, scheduling, and transport controls.

Sources: [Tone.js Documentation](https://tonejs.github.io/), [GitHub - Tone.js](https://github.com/Tonejs/Tone.js)

#### 2.1 Ambient Pads with AMSynth and FMSynth

```javascript
import * as Tone from 'tone';

// Create a rich ambient pad using FM synthesis
const pad = new Tone.FMSynth({
  harmonicity: 3.01,      // ratio between carrier and modulator
  modulationIndex: 14,     // depth of FM modulation
  oscillator: { type: 'sine' },
  envelope: {
    attack: 4,             // 4 second fade in
    decay: 2,
    sustain: 0.8,
    release: 8             // 8 second fade out
  },
  modulation: { type: 'triangle' },
  modulationEnvelope: {
    attack: 3,
    decay: 1,
    sustain: 0.5,
    release: 6
  }
}).toDestination();

// AM synthesis for shimmering textures
const shimmer = new Tone.AMSynth({
  harmonicity: 2.5,
  oscillator: { type: 'sine' },
  envelope: {
    attack: 3,
    decay: 1,
    sustain: 0.7,
    release: 6
  },
  modulation: {
    type: 'sine'
  },
  modulationEnvelope: {
    attack: 2,
    decay: 0.5,
    sustain: 0.4,
    release: 4
  }
}).toDestination();
```

#### 2.2 Effects Chains

```javascript
// Create ambient effects chain
const reverb = new Tone.Reverb({
  decay: 12,        // 12-second reverb tail
  wet: 0.8,
  preDelay: 0.1
});
await reverb.generate(); // must generate impulse response

const delay = new Tone.FeedbackDelay({
  delayTime: '4n.',      // dotted quarter note
  feedback: 0.35,
  wet: 0.4
});

const chorus = new Tone.Chorus({
  frequency: 0.3,        // very slow chorus
  delayTime: 12,
  depth: 0.8,
  wet: 0.5
});

const filter = new Tone.AutoFilter({
  frequency: 0.08,       // one cycle every ~12 seconds
  baseFrequency: 200,
  octaves: 4,
  wet: 1
}).start();

// Chain: synth -> chorus -> delay -> reverb -> output
const synth = new Tone.PolySynth(Tone.FMSynth, {
  envelope: { attack: 3, decay: 1, sustain: 0.6, release: 5 }
});

synth.chain(chorus, filter, delay, reverb, Tone.getDestination());
```

#### 2.3 Scheduling with Transport and Patterns

```javascript
// Generative ambient sequence using Tone.Transport
const scale = ['C3', 'D3', 'E3', 'G3', 'A3', 'C4', 'D4', 'E4', 'G4', 'A4'];

// Random note selection with variable timing
function scheduleNextNote() {
  const note = scale[Math.floor(Math.random() * scale.length)];
  const duration = Math.random() * 6 + 2; // 2-8 seconds
  const velocity = Math.random() * 0.3 + 0.1; // soft dynamics

  synth.triggerAttackRelease(note, duration, undefined, velocity);

  // Schedule next note with variable interval (3-10 seconds)
  const nextTime = Math.random() * 7 + 3;
  setTimeout(scheduleNextNote, nextTime * 1000);
}

// Tone.Pattern for more structured ambient sequences
const pattern = new Tone.Pattern((time, note) => {
  synth.triggerAttackRelease(note, '2n', time, 0.2);
}, ['C3', 'E3', 'G3', 'A3', 'D4'], 'randomWalk');

pattern.interval = '2n';
pattern.humanize = true; // adds slight timing variation

Tone.getTransport().bpm.value = 30; // very slow tempo
Tone.getTransport().start();
pattern.start();
```

#### 2.4 Eno-Style Tape Loop System in Tone.js

Recreating the *Music for Airports* incommensurable loop technique:

```javascript
import * as Tone from 'tone';

class EnoLoopSystem {
  constructor() {
    this.loops = [];
    this.reverb = new Tone.Reverb({ decay: 8, wet: 0.7 });
  }

  async init() {
    await this.reverb.generate();
    await Tone.start();
  }

  addLoop(note, durationSeconds, volume = 0.3) {
    const synth = new Tone.Synth({
      oscillator: { type: 'sine' },
      envelope: { attack: 1.5, decay: 0.5, sustain: 0.6, release: 3 }
    }).connect(this.reverb).toDestination();

    // Each loop has its own independent interval
    const loop = new Tone.Loop((time) => {
      synth.triggerAttackRelease(note, '2n', time, volume);
    }, durationSeconds);

    this.loops.push(loop);
    return loop;
  }

  start() {
    // Incommensurable loop lengths (in seconds)
    this.addLoop('Ab4', 17.8);
    this.addLoop('Eb4', 16.2);
    this.addLoop('F4', 19.6);
    this.addLoop('C4', 20.1);
    this.addLoop('Ab3', 21.3);
    this.addLoop('F3', 24.7);
    this.addLoop('Db4', 31.8);

    this.loops.forEach(loop => loop.start(0));
    Tone.getTransport().start();
  }

  stop() {
    this.loops.forEach(loop => loop.stop());
    Tone.getTransport().stop();
  }
}

// Usage
const eno = new EnoLoopSystem();
await eno.init();
eno.start();
```

Sources: [Tone.js - AMSynth Docs](https://tonejs.github.io/docs/AMSynth), [Tone.js - FeedbackDelay Docs](https://tonejs.github.io/docs/r13/FeedbackDelay), [GuitarLand - Tone.js Music Theory](https://www.guitarland.com/MusicTheoryWithToneJS/EditSynth.html)

---

### 3. Generative and Algorithmic Approaches

#### 3.1 Markov Chains for Melody Generation

A Markov chain defines transition probabilities between states (notes or chords). Given the current note, it probabilistically selects the next note based on learned or hand-crafted transition weights.

```javascript
class MarkovMelody {
  constructor(scale, transitionWeights = null) {
    this.scale = scale;
    this.currentIndex = Math.floor(Math.random() * scale.length);

    // Default: favor stepwise motion
    if (!transitionWeights) {
      this.transitionMatrix = this.buildDefaultMatrix();
    } else {
      this.transitionMatrix = transitionWeights;
    }
  }

  buildDefaultMatrix() {
    const n = this.scale.length;
    const matrix = [];

    for (let i = 0; i < n; i++) {
      const row = [];
      for (let j = 0; j < n; j++) {
        const distance = Math.abs(i - j);
        // Higher probability for nearby notes, with slight
        // preference for upward motion
        if (distance === 0) row.push(0.05);       // repeat: rare
        else if (distance === 1) row.push(0.35);   // step: common
        else if (distance === 2) row.push(0.25);   // skip: likely
        else if (distance === 3) row.push(0.15);   // third: moderate
        else row.push(0.05);                        // leap: rare
      }
      // Normalize row
      const sum = row.reduce((a, b) => a + b, 0);
      matrix.push(row.map(w => w / sum));
    }
    return matrix;
  }

  nextNote() {
    const weights = this.transitionMatrix[this.currentIndex];
    let random = Math.random();

    for (let i = 0; i < weights.length; i++) {
      random -= weights[i];
      if (random <= 0) {
        this.currentIndex = i;
        return this.scale[i];
      }
    }

    this.currentIndex = this.scale.length - 1;
    return this.scale[this.currentIndex];
  }
}

// Usage with Tone.js
const scale = ['C3', 'D3', 'E3', 'G3', 'A3', 'C4', 'D4', 'E4'];
const melody = new MarkovMelody(scale);

const synth = new Tone.Synth({
  envelope: { attack: 2, decay: 1, sustain: 0.5, release: 4 }
}).toDestination();

function playNext() {
  const note = melody.nextNote();
  const duration = 2 + Math.random() * 4; // 2-6 seconds
  synth.triggerAttackRelease(note, duration);

  setTimeout(playNext, (duration + Math.random() * 3) * 1000);
}
playNext();
```

Sources: [HackerNoon - Generating Music Using Markov Chains](https://hackernoon.com/generating-music-using-markov-chains-40c3f3f46405), [GitHub - markov-chain-music-generator](https://github.com/conzmr/markov-chain-music-generator)

#### 3.2 L-Systems for Musical Structures

Lindenmayer Systems (L-systems) use recursive string rewriting to generate fractal-like structures that can be mapped to musical parameters.

```javascript
class LSystemMusic {
  constructor(axiom, rules, iterations) {
    this.axiom = axiom;
    this.rules = rules;
    this.iterations = iterations;
  }

  generate() {
    let current = this.axiom;

    for (let i = 0; i < this.iterations; i++) {
      let next = '';
      for (const char of current) {
        next += this.rules[char] || char;
      }
      current = next;
    }
    return current;
  }

  toNotes(scaleNotes) {
    const result = this.generate();
    const notes = [];
    let pitchIndex = Math.floor(scaleNotes.length / 2);
    let duration = 2;

    for (const char of result) {
      switch (char) {
        case 'A': // play current note
          notes.push({
            note: scaleNotes[pitchIndex % scaleNotes.length],
            duration
          });
          break;
        case 'B': // play and move up
          pitchIndex = Math.min(pitchIndex + 1, scaleNotes.length - 1);
          notes.push({
            note: scaleNotes[pitchIndex],
            duration
          });
          break;
        case '+': // move up without playing
          pitchIndex = Math.min(pitchIndex + 1, scaleNotes.length - 1);
          break;
        case '-': // move down without playing
          pitchIndex = Math.max(pitchIndex - 1, 0);
          break;
        case '[': // longer note
          duration *= 1.5;
          break;
        case ']': // shorter note
          duration /= 1.5;
          break;
      }
    }
    return notes;
  }
}

// Example: generates a slowly expanding then contracting melody
const lsys = new LSystemMusic(
  'A',
  {
    'A': 'A+B[A]-A',
    'B': 'B-A[B]+B'
  },
  3
);

const scale = ['C3', 'D3', 'E3', 'G3', 'A3', 'C4', 'D4', 'E4', 'G4'];
const noteSequence = lsys.toNotes(scale);
```

Sources: [ResearchGate - L-systems for Rhythmic Sequences](https://www.researchgate.net/publication/230833320_Intelligent_Generation_of_Rhythmic_Sequences_Using_Finite_L-systems), [Algorithmic Composition Tutorial](https://junshern.github.io/algorithmic-music-tutorial/part1.html)

#### 3.3 Perlin Noise for Parameter Modulation

Perlin noise generates smooth, organic-looking random values -- ideal for modulating ambient music parameters like filter cutoff, volume, panning, and pitch.

```javascript
// Simplified 1D Perlin noise (for production, use a library like noisejs)
class PerlinModulator {
  constructor() {
    this.permutation = Array.from({ length: 256 }, (_, i) => i);
    // Fisher-Yates shuffle
    for (let i = 255; i > 0; i--) {
      const j = Math.floor(Math.random() * (i + 1));
      [this.permutation[i], this.permutation[j]] =
        [this.permutation[j], this.permutation[i]];
    }
    this.p = [...this.permutation, ...this.permutation];
  }

  fade(t) { return t * t * t * (t * (t * 6 - 15) + 10); }
  lerp(a, b, t) { return a + t * (b - a); }
  grad(hash, x) { return (hash & 1) === 0 ? x : -x; }

  noise(x) {
    const xi = Math.floor(x) & 255;
    const xf = x - Math.floor(x);
    const u = this.fade(xf);
    return this.lerp(
      this.grad(this.p[xi], xf),
      this.grad(this.p[xi + 1], xf - 1),
      u
    );
  }

  // Get a smoothly varying value over time
  getValue(time, speed = 0.1, octaves = 3) {
    let value = 0;
    let amplitude = 1;
    let frequency = speed;
    let maxValue = 0;

    for (let i = 0; i < octaves; i++) {
      value += this.noise(time * frequency) * amplitude;
      maxValue += amplitude;
      amplitude *= 0.5;
      frequency *= 2;
    }

    return value / maxValue; // normalized to [-1, 1]
  }
}

// Use Perlin noise to modulate audio parameters
const perlin = new PerlinModulator();
const ctx = new AudioContext();

function modulateWithPerlin(audioParam, min, max, speed) {
  let startTime = performance.now();

  function update() {
    const elapsed = (performance.now() - startTime) / 1000;
    const noiseValue = perlin.getValue(elapsed, speed);
    // Map from [-1, 1] to [min, max]
    const mapped = min + (noiseValue + 1) * 0.5 * (max - min);
    audioParam.setValueAtTime(mapped, ctx.currentTime);
    requestAnimationFrame(update);
  }
  update();
}

// Example: Perlin-modulated filter sweep
const osc = ctx.createOscillator();
const filter = ctx.createBiquadFilter();
osc.type = 'sawtooth';
osc.frequency.value = 55;
filter.type = 'lowpass';
filter.Q.value = 2;
osc.connect(filter);
filter.connect(ctx.destination);
osc.start();

modulateWithPerlin(filter.frequency, 100, 2000, 0.05);
```

Sources: [GitHub - noisejs](https://github.com/josephg/noisejs), [SoundCy - Procedural Sound Generation](https://soundcy.com/article/how-to-procedurally-generate-sound)

#### 3.4 Cellular Automata for Pattern Generation

Cellular automata (like Conway's Game of Life or elementary automata) can generate patterns that map to musical events -- cells become notes, rows become time steps.

```javascript
class CellularAutomataMusic {
  constructor(width = 16, rule = 110) {
    this.width = width;
    this.rule = rule;
    this.state = new Array(width).fill(0);
    // Seed with single cell in the center
    this.state[Math.floor(width / 2)] = 1;
  }

  step() {
    const newState = new Array(this.width).fill(0);

    for (let i = 0; i < this.width; i++) {
      const left = this.state[(i - 1 + this.width) % this.width];
      const center = this.state[i];
      const right = this.state[(i + 1) % this.width];
      const pattern = (left << 2) | (center << 1) | right;
      newState[i] = (this.rule >> pattern) & 1;
    }

    this.state = newState;
    return this.state;
  }

  toNotes(scale) {
    const notes = [];
    this.state.forEach((cell, i) => {
      if (cell === 1) {
        notes.push(scale[i % scale.length]);
      }
    });
    return notes;
  }
}

// Usage: each automaton step produces a set of simultaneous notes
const ca = new CellularAutomataMusic(12, 110);
const scale = ['C3', 'D3', 'E3', 'G3', 'A3', 'C4',
               'D4', 'E4', 'G4', 'A4', 'C5', 'D5'];

function playStep() {
  ca.step();
  const notes = ca.toNotes(scale);

  notes.forEach(note => {
    synth.triggerAttackRelease(note, '2n', undefined, 0.15);
  });

  setTimeout(playStep, 4000); // one step every 4 seconds
}
playStep();
```

Sources: [Medium - Listening to Elementary Cellular Automata](https://medium.com/code-music-noise/listening-to-elementary-cellular-automata-661018229362), [GitHub - music-of-life](https://github.com/plhosk/music-of-life)

#### 3.5 Oblique Strategies in Code

Eno's Oblique Strategies can be implemented as algorithmic constraints that periodically alter a generative system's parameters.

```javascript
const obliqueStrategies = [
  { name: 'Thin out', action: (params) => { params.density *= 0.5; } },
  { name: 'Fatten', action: (params) => { params.density *= 1.5; } },
  { name: 'Reverse', action: (params) => { params.scale.reverse(); } },
  { name: 'Transpose up', action: (params) => { params.rootMidi += 7; } },
  { name: 'Transpose down', action: (params) => { params.rootMidi -= 5; } },
  { name: 'Simplify', action: (params) => {
    params.scale = params.scale.filter((_, i) => i % 2 === 0);
  }},
  { name: 'Add complexity', action: (params) => {
    params.scale.push(params.rootMidi + 6); // add tritone
  }},
  { name: 'Change timbre', action: (params) => {
    const types = ['sine', 'triangle', 'sawtooth'];
    params.waveform = types[Math.floor(Math.random() * types.length)];
  }},
  { name: 'Slow down', action: (params) => { params.tempo *= 0.7; } },
  { name: 'Speed up', action: (params) => { params.tempo *= 1.3; } },
  { name: 'Go silent briefly', action: (params) => { params.mute = true; } },
  { name: 'Use only high notes', action: (params) => {
    params.scale = params.scale.filter(n => n > 60);
  }},
];

function applyRandomStrategy(params) {
  const strategy = obliqueStrategies[
    Math.floor(Math.random() * obliqueStrategies.length)
  ];
  console.log(`Applying: "${strategy.name}"`);
  strategy.action(params);
  return strategy.name;
}

// Apply a random strategy every 30-90 seconds
function scheduleStrategies(params) {
  const interval = 30000 + Math.random() * 60000;
  setTimeout(() => {
    applyRandomStrategy(params);
    scheduleStrategies(params);
  }, interval);
}
```

---

### 4. Notable Web-Based Ambient Projects

#### Generative.fm
[generative.fm](https://generative.fm/) by Alex Bainter is the most prominent web-based generative ambient platform. Key architectural details:

- **Audio engine:** Built on Tone.js, using the Web Audio API for all synthesis and scheduling
- **Sample sources:** Primarily uses Versilian Studios Chamber Orchestra 2 Community Edition and Sonatina Symphonic Orchestra samples
- **Architecture:** Progressive Web App (PWA) with service worker caching for offline playback; audio samples cached on first request
- **State management:** Redux with localStorage persistence for favorites and listening history
- **Music model:** Each "piece" is a self-contained generative system; composition is by a human, but every performance is unique and can sustain indefinitely

Sources: [Generative.fm](https://generative.fm/), [Alex Bainter - How to Host a Generative Music Platform on the Web](https://medium.com/@alexbainter/how-to-host-a-generative-music-platform-on-the-web-3c71e25b225a)

#### Brian Eno's Reflection App
As discussed in Part I, the *Reflection* app (by Eno and Peter Chilvers) is a generative system that plays indefinitely, with algorithms modulating musical output based on time of day. Available on iOS and Apple TV.

Sources: [GenerativeMusic.com - Reflection](https://www.generativemusic.com/reflection.html)

#### Chrome Music Lab
[Chrome Music Lab](https://musiclab.chromeexperiments.com/) by Google Creative Lab is a collection of 14 web-based audio experiments built entirely with the Web Audio API. While not specifically ambient, experiments like *Spectrogram*, *Oscillators*, and *Sound Waves* demonstrate interactive browser audio principles that are directly applicable to ambient generation. The entire project is [open source](https://github.com/googlecreativelab/chrome-music-lab).

Sources: [Chrome Music Lab](https://musiclab.chromeexperiments.com/), [GitHub - Chrome Music Lab](https://github.com/googlecreativelab/chrome-music-lab)

#### Patatap
[Patatap](https://patatap.com/) maps every letter key on the keyboard to a different synth sound with an accompanying animation. While more percussive than ambient, it demonstrates real-time browser audio synthesis triggered by user interaction.

#### Other Notable Projects
- **ZYA Granular Synthesiser** -- a browser-based granular synth with multi-touch support using Web Audio API and Processing.js ([Demo](https://zya.github.io/granular/))
- **Teropa's "How Generative Music Works"** -- an interactive essay with playable examples demonstrating Eno's generative techniques in the browser ([teropa.info/loop](https://teropa.info/loop/))
- **pparocza/generative-music-web-audio** -- custom synthesizers and compositional algorithms built in JavaScript's Web Audio API, featuring architectures modeled on the Yamaha DX7 and Moog MiniMoog ([GitHub](https://github.com/pparocza/generative-music-web-audio))

Sources: [Submarine Channel - Top 5 Interactive Generative Music Sites](https://submarinechannel.com/top5/top-5-of-our-favourite-things-interactive-generative-music-sites-2/), [GitHub - awesome-webaudio](https://github.com/notthetup/awesome-webaudio)

---

### 5. Interactive Ambient for Websites

#### 5.1 Reactive Ambient: Responding to User Interaction

```javascript
class ReactiveAmbient {
  constructor() {
    this.ctx = new AudioContext();
    this.isActive = false;
  }

  async init() {
    await this.ctx.resume();
    this.isActive = true;

    // Create base layers
    this.drone = this.createDrone(55); // A1
    this.filter = this.ctx.createBiquadFilter();
    this.filter.type = 'lowpass';
    this.filter.frequency.value = 300;
    this.drone.connect(this.filter);
    this.filter.connect(this.ctx.destination);

    this.setupScrollReaction();
    this.setupMouseReaction();
    this.setupTimeOfDayReaction();
  }

  createDrone(freq) {
    const osc1 = this.ctx.createOscillator();
    const osc2 = this.ctx.createOscillator();
    const gain = this.ctx.createGain();

    osc1.frequency.value = freq;
    osc2.frequency.value = freq * 1.003; // slight detune
    osc1.type = 'sine';
    osc2.type = 'triangle';

    gain.gain.value = 0.15;
    osc1.connect(gain);
    osc2.connect(gain);
    osc1.start();
    osc2.start();

    return gain;
  }

  setupScrollReaction() {
    // Scroll position controls filter cutoff
    window.addEventListener('scroll', () => {
      if (!this.isActive) return;
      const scrollPercent = window.scrollY /
        (document.body.scrollHeight - window.innerHeight);
      // Map scroll 0-100% to filter 200-4000 Hz
      const freq = 200 + scrollPercent * 3800;
      this.filter.frequency.exponentialRampToValueAtTime(
        freq, this.ctx.currentTime + 0.1
      );
    });
  }

  setupMouseReaction() {
    // Mouse X controls stereo panning, Y controls pitch
    window.addEventListener('mousemove', (e) => {
      if (!this.isActive) return;
      const xNorm = (e.clientX / window.innerWidth) * 2 - 1; // -1 to 1
      const yNorm = e.clientY / window.innerHeight; // 0 to 1

      // Use Y position to modulate filter Q (resonance)
      this.filter.Q.setValueAtTime(
        yNorm * 8, this.ctx.currentTime
      );
    });
  }

  setupTimeOfDayReaction() {
    // Adjust character based on time of day (Eno's Reflection approach)
    const hour = new Date().getHours();

    if (hour >= 6 && hour < 12) {
      // Morning: brighter, higher filter
      this.filter.frequency.value = 2000;
      this.drone.gain.value = 0.1;
    } else if (hour >= 12 && hour < 18) {
      // Afternoon: medium
      this.filter.frequency.value = 800;
      this.drone.gain.value = 0.15;
    } else if (hour >= 18 && hour < 22) {
      // Evening: warmer, lower filter
      this.filter.frequency.value = 400;
      this.drone.gain.value = 0.18;
    } else {
      // Night: dark, minimal
      this.filter.frequency.value = 200;
      this.drone.gain.value = 0.08;
    }
  }
}
```

#### 5.2 Spatial Audio with PannerNode

```javascript
class SpatialAmbient {
  constructor(ctx) {
    this.ctx = ctx;
    // Set up listener position
    this.ctx.listener.positionX.value = 0;
    this.ctx.listener.positionY.value = 0;
    this.ctx.listener.positionZ.value = 0;
  }

  createSpatialSource(freq, x, y, z) {
    const osc = this.ctx.createOscillator();
    const gain = this.ctx.createGain();
    const panner = this.ctx.createPanner();

    panner.panningModel = 'HRTF'; // head-related transfer function
    panner.distanceModel = 'exponential';
    panner.refDistance = 1;
    panner.maxDistance = 50;
    panner.rolloffFactor = 1.5;

    // Position the sound source in 3D space
    panner.positionX.value = x;
    panner.positionY.value = y;
    panner.positionZ.value = z;

    osc.frequency.value = freq;
    osc.type = 'sine';
    gain.gain.value = 0.2;

    osc.connect(gain);
    gain.connect(panner);
    panner.connect(this.ctx.destination);
    osc.start();

    return { osc, gain, panner };
  }

  // Create orbiting sound sources
  createOrbitingSource(freq, radius, speed) {
    const source = this.createSpatialSource(freq, radius, 0, 0);
    let angle = Math.random() * Math.PI * 2;

    const orbit = () => {
      angle += speed;
      source.panner.positionX.setValueAtTime(
        Math.cos(angle) * radius, this.ctx.currentTime
      );
      source.panner.positionZ.setValueAtTime(
        Math.sin(angle) * radius, this.ctx.currentTime
      );
      requestAnimationFrame(orbit);
    };
    orbit();

    return source;
  }
}

// Usage: create a spatial ambient environment
const ctx = new AudioContext();
const spatial = new SpatialAmbient(ctx);

// Place drones at different positions orbiting the listener
spatial.createOrbitingSource(110, 5, 0.001);   // slow orbit, 5m radius
spatial.createOrbitingSource(165, 8, 0.0007);  // slower, wider orbit
spatial.createOrbitingSource(220, 3, 0.002);   // faster, closer orbit
```

Sources: [MDN - Web Audio Spatialization Basics](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Web_audio_spatialization_basics), [DZone - Implementing Spatial Audio](https://dzone.com/articles/implementing-spatial-audio-with-web-audio-api)

#### 5.3 Binaural Beats

Binaural beats create a perceived "third tone" when slightly different frequencies are sent to each ear. A 100 Hz tone in the left ear and 104 Hz in the right produces a perceived 4 Hz beat (associated with theta brainwave states and deep relaxation).

```javascript
class BinauralBeatGenerator {
  constructor() {
    this.ctx = new AudioContext();
    this.merger = this.ctx.createChannelMerger(2);
    this.merger.connect(this.ctx.destination);
  }

  createBeat(baseFreq, beatFreq, volume = 0.2) {
    // Left ear: base frequency
    const oscLeft = this.ctx.createOscillator();
    const gainLeft = this.ctx.createGain();
    oscLeft.frequency.value = baseFreq;
    gainLeft.gain.value = volume;
    oscLeft.connect(gainLeft);
    gainLeft.connect(this.merger, 0, 0); // channel 0 = left

    // Right ear: base + beat frequency
    const oscRight = this.ctx.createOscillator();
    const gainRight = this.ctx.createGain();
    oscRight.frequency.value = baseFreq + beatFreq;
    gainRight.gain.value = volume;
    oscRight.connect(gainRight);
    gainRight.connect(this.merger, 0, 1); // channel 1 = right

    oscLeft.start();
    oscRight.start();

    return { oscLeft, oscRight, gainLeft, gainRight };
  }

  // Preset binaural beat frequencies (Hz)
  static PRESETS = {
    delta: 2,     // deep sleep (0.5-4 Hz)
    theta: 6,     // meditation, creativity (4-8 Hz)
    alpha: 10,    // relaxation, calm focus (8-13 Hz)
    beta: 20,     // active thinking (13-30 Hz)
  };

  createPreset(baseFreq, presetName) {
    const beatFreq = BinauralBeatGenerator.PRESETS[presetName];
    if (!beatFreq) throw new Error(`Unknown preset: ${presetName}`);
    return this.createBeat(baseFreq, beatFreq);
  }
}

// Usage: theta binaural beat for meditation
const bb = new BinauralBeatGenerator();
bb.createPreset(200, 'theta'); // 200 Hz left, 206 Hz right
```

Sources: [GitHub - BinauralBeatJS](https://github.com/ichabodcole/BinauralBeatJS), [PureBinaural](https://purebinaural.com/)

#### 5.4 Integration with Visuals (Canvas/WebGL)

Audio-reactive visuals create a unified ambient experience. Use `AnalyserNode` to extract frequency and waveform data for driving visual elements.

```javascript
class AmbientVisualizer {
  constructor(canvasId, audioSource, ctx) {
    this.canvas = document.getElementById(canvasId);
    this.canvasCtx = this.canvas.getContext('2d');
    this.audioCtx = ctx;

    // Create analyser
    this.analyser = this.audioCtx.createAnalyser();
    this.analyser.fftSize = 2048;
    this.analyser.smoothingTimeConstant = 0.9; // very smooth for ambient

    audioSource.connect(this.analyser);
    this.analyser.connect(this.audioCtx.destination);

    this.frequencyData = new Uint8Array(this.analyser.frequencyBinCount);
    this.waveformData = new Uint8Array(this.analyser.fftSize);
  }

  draw() {
    this.analyser.getByteFrequencyData(this.frequencyData);
    this.analyser.getByteTimeDomainData(this.waveformData);

    const w = this.canvas.width;
    const h = this.canvas.height;

    // Slow fade effect (trailing)
    this.canvasCtx.fillStyle = 'rgba(0, 0, 0, 0.02)';
    this.canvasCtx.fillRect(0, 0, w, h);

    // Draw frequency bands as slowly pulsing circles
    const bands = 8;
    const bandSize = Math.floor(this.frequencyData.length / bands);

    for (let i = 0; i < bands; i++) {
      let sum = 0;
      for (let j = 0; j < bandSize; j++) {
        sum += this.frequencyData[i * bandSize + j];
      }
      const avg = sum / bandSize / 255;

      const x = w * (i + 0.5) / bands;
      const y = h / 2;
      const radius = avg * h * 0.3 + 10;

      this.canvasCtx.beginPath();
      this.canvasCtx.arc(x, y, radius, 0, Math.PI * 2);
      this.canvasCtx.fillStyle =
        `hsla(${200 + i * 20}, 60%, ${30 + avg * 40}%, ${avg * 0.3})`;
      this.canvasCtx.fill();
    }

    requestAnimationFrame(() => this.draw());
  }

  start() {
    this.canvas.width = window.innerWidth;
    this.canvas.height = window.innerHeight;
    this.draw();
  }
}
```

---

### 6. Complete Working Examples

#### 6.1 Simple Generative Ambient Drone

A self-contained ambient drone that creates layered oscillators with slow modulation, reverb, and evolving timbral movement. Paste this into an HTML file and open in a browser.

```html
<!DOCTYPE html>
<html>
<head><title>Ambient Drone</title></head>
<body style="background:#111;color:#ccc;font-family:sans-serif;
  display:flex;align-items:center;justify-content:center;height:100vh">
  <button id="start" style="padding:20px 40px;font-size:18px;
    cursor:pointer;background:#222;color:#8af;border:1px solid #8af;
    border-radius:8px">
    Start Ambient Drone
  </button>
<script>
document.getElementById('start').addEventListener('click', async () => {
  const ctx = new AudioContext();
  const master = ctx.createGain();
  master.gain.value = 0.25;
  master.connect(ctx.destination);

  // Synthetic reverb
  const convolver = ctx.createConvolver();
  const reverbLen = ctx.sampleRate * 5;
  const impulse = ctx.createBuffer(2, reverbLen, ctx.sampleRate);
  for (let ch = 0; ch < 2; ch++) {
    const d = impulse.getChannelData(ch);
    for (let i = 0; i < reverbLen; i++) {
      d[i] = (Math.random() * 2 - 1) * Math.pow(1 - i / reverbLen, 2.5);
    }
  }
  convolver.buffer = impulse;
  convolver.connect(master);

  function createOsc(freq, detune, type, vol) {
    const o = ctx.createOscillator();
    const g = ctx.createGain();
    const f = ctx.createBiquadFilter();
    o.type = type;
    o.frequency.value = freq;
    o.detune.value = detune;
    f.type = 'lowpass';
    f.frequency.value = 600;
    f.Q.value = 1;
    g.gain.value = 0;
    g.gain.linearRampToValueAtTime(vol, ctx.currentTime + 5);
    o.connect(f);
    f.connect(g);
    g.connect(convolver);
    o.start();
    return { o, g, f };
  }

  function createLFO(param, rate, depth) {
    const lfo = ctx.createOscillator();
    const lg = ctx.createGain();
    lfo.frequency.value = rate;
    lg.gain.value = depth;
    lfo.connect(lg);
    lg.connect(param);
    lfo.start();
  }

  // Build drone layers
  const root = 55; // A1
  const layers = [
    createOsc(root, 0, 'sine', 0.15),
    createOsc(root, 7, 'sine', 0.12),
    createOsc(root, -5, 'triangle', 0.08),
    createOsc(root * 2, 3, 'sine', 0.06),
    createOsc(root * 1.5, -4, 'sine', 0.05),
    createOsc(root * 3, 10, 'sine', 0.03),
    createOsc(root / 2, 0, 'sine', 0.1),
  ];

  // Add slow modulation to each layer
  layers.forEach((layer, i) => {
    createLFO(layer.f.frequency, 0.02 + i * 0.007, 150 + i * 50);
    createLFO(layer.o.detune, 0.01 + i * 0.005, 8);
    createLFO(layer.g.gain, 0.03 + i * 0.01, 0.03);
  });

  document.getElementById('start').textContent = 'Drone Active';
});
</script>
</body>
</html>
```

#### 6.2 Pentatonic Ambient Melody Generator

Generates an endless ambient melody from a pentatonic scale with reverb, delay, and randomized timing.

```html
<!DOCTYPE html>
<html>
<head><title>Ambient Melody</title></head>
<body style="background:#0a0a1a;color:#aac;font-family:sans-serif;
  display:flex;align-items:center;justify-content:center;height:100vh">
  <button id="start" style="padding:20px 40px;font-size:18px;
    cursor:pointer;background:#111;color:#8cf;border:1px solid #8cf;
    border-radius:8px">
    Start Ambient Melody
  </button>
<script>
document.getElementById('start').addEventListener('click', async () => {
  const ctx = new AudioContext();

  // Reverb
  const conv = ctx.createConvolver();
  const irLen = ctx.sampleRate * 6;
  const ir = ctx.createBuffer(2, irLen, ctx.sampleRate);
  for (let ch = 0; ch < 2; ch++) {
    const d = ir.getChannelData(ch);
    for (let i = 0; i < irLen; i++) {
      d[i] = (Math.random() * 2 - 1) * Math.pow(1 - i / irLen, 3);
    }
  }
  conv.buffer = ir;

  // Delay
  const delay = ctx.createDelay(4);
  delay.delayTime.value = 2.0;
  const fb = ctx.createGain();
  fb.gain.value = 0.3;
  const delayFilter = ctx.createBiquadFilter();
  delayFilter.type = 'lowpass';
  delayFilter.frequency.value = 1500;
  delay.connect(delayFilter);
  delayFilter.connect(fb);
  fb.connect(delay);

  // Output mixing
  const master = ctx.createGain();
  master.gain.value = 0.4;
  conv.connect(master);
  delay.connect(master);
  master.connect(ctx.destination);

  // Pentatonic scale: C, D, E, G, A across 3 octaves
  const pentatonic = [0, 2, 4, 7, 9];
  const notes = [];
  for (let oct = 3; oct <= 5; oct++) {
    for (const interval of pentatonic) {
      notes.push(440 * Math.pow(2, (((oct - 4) * 12 + interval) - 9) / 12));
    }
  }

  let currentIndex = Math.floor(notes.length / 2);

  function playNote() {
    // Weighted random walk: favor nearby notes
    const weights = notes.map((_, i) => {
      const dist = Math.abs(i - currentIndex);
      return Math.exp(-dist * 0.4);
    });
    const total = weights.reduce((a, b) => a + b, 0);
    let r = Math.random() * total;
    for (let i = 0; i < weights.length; i++) {
      r -= weights[i];
      if (r <= 0) { currentIndex = i; break; }
    }

    const freq = notes[currentIndex];
    const osc = ctx.createOscillator();
    const gain = ctx.createGain();
    const panner = ctx.createStereoPanner();

    osc.type = Math.random() > 0.7 ? 'triangle' : 'sine';
    osc.frequency.value = freq;
    panner.pan.value = Math.random() * 1.6 - 0.8;

    const dur = 2 + Math.random() * 4;
    const now = ctx.currentTime;
    gain.gain.setValueAtTime(0, now);
    gain.gain.linearRampToValueAtTime(0.12, now + 0.8);
    gain.gain.exponentialRampToValueAtTime(0.001, now + dur);

    osc.connect(gain);
    gain.connect(panner);
    panner.connect(conv);
    panner.connect(delay);
    osc.start(now);
    osc.stop(now + dur + 0.1);

    // Schedule next note: 1.5 - 6 seconds
    const nextDelay = 1500 + Math.random() * 4500;
    setTimeout(playNote, nextDelay);
  }

  playNote();
  document.getElementById('start').textContent = 'Melody Active';
});
</script>
</body>
</html>
```

#### 6.3 Granular Texture Engine

A granular synthesis engine that chops an audio buffer into tiny grains and reassembles them with randomized parameters for evolving textures.

```javascript
class GranularEngine {
  constructor(ctx) {
    this.ctx = ctx;
    this.buffer = null;
    this.isPlaying = false;
    this.output = ctx.createGain();
    this.output.gain.value = 0.5;
    this.output.connect(ctx.destination);
  }

  async loadSample(url) {
    const response = await fetch(url);
    const arrayBuffer = await response.arrayBuffer();
    this.buffer = await this.ctx.decodeAudioData(arrayBuffer);
  }

  playGrain(position, duration, pitch, pan, volume) {
    if (!this.buffer) return;

    const source = this.ctx.createBufferSource();
    const gain = this.ctx.createGain();
    const panner = this.ctx.createStereoPanner();

    source.buffer = this.buffer;
    source.playbackRate.value = pitch;

    // Grain envelope: attack 20%, sustain 60%, release 20%
    const now = this.ctx.currentTime;
    const attack = duration * 0.2;
    const release = duration * 0.2;

    gain.gain.setValueAtTime(0, now);
    gain.gain.linearRampToValueAtTime(volume, now + attack);
    gain.gain.setValueAtTime(volume, now + duration - release);
    gain.gain.linearRampToValueAtTime(0, now + duration);

    panner.pan.value = pan;

    source.connect(gain);
    gain.connect(panner);
    panner.connect(this.output);

    // Start playback from the position in the buffer
    const offset = position * this.buffer.duration;
    source.start(now, offset, duration);
  }

  start(params = {}) {
    const {
      grainSize = 0.1,     // grain duration in seconds
      density = 10,         // grains per second
      spread = 0.3,         // position randomness (0-1)
      pitchVariation = 0.1, // pitch randomness
      basePitch = 1.0,      // base playback rate
      scanSpeed = 0.01,     // how fast we move through the buffer
    } = params;

    this.isPlaying = true;
    let position = 0;

    const scheduleGrain = () => {
      if (!this.isPlaying) return;

      // Randomize grain parameters
      const pos = Math.max(0, Math.min(1,
        position + (Math.random() - 0.5) * spread
      ));
      const dur = grainSize * (0.5 + Math.random());
      const pitch = basePitch + (Math.random() - 0.5) * pitchVariation;
      const pan = Math.random() * 2 - 1;
      const vol = 0.1 + Math.random() * 0.15;

      this.playGrain(pos, dur, pitch, pan, vol);

      // Advance position
      position = (position + scanSpeed) % 1;

      // Schedule next grain
      const interval = 1000 / density;
      const jitter = interval * 0.3;
      setTimeout(scheduleGrain, interval + (Math.random() - 0.5) * jitter);
    };

    scheduleGrain();
  }

  stop() {
    this.isPlaying = false;
  }
}

// Usage:
// const ctx = new AudioContext();
// const granular = new GranularEngine(ctx);
// await granular.loadSample('field-recording.mp3');
// granular.start({
//   grainSize: 0.08,
//   density: 15,
//   spread: 0.2,
//   pitchVariation: 0.05,
//   scanSpeed: 0.005,
// });
```

Sources: [DEV Community - Granular Synthesis in the Browser](https://dev.to/hexshift/granular-synthesis-in-the-browser-using-web-audio-api-and-audiobuffer-slicing-2o9h), [GitHub - granular-js](https://github.com/philippfromme/granular-js), [ZYA Granular Synthesiser](https://zya.github.io/granular/)

#### 6.4 Ambient Pad with Slow Filter Sweeps

A lush ambient pad synthesizer using multiple detuned oscillators with Perlin-noise-driven filter modulation.

```html
<!DOCTYPE html>
<html>
<head><title>Ambient Pad</title></head>
<body style="background:#0a0a12;color:#aab;font-family:sans-serif;
  display:flex;flex-direction:column;align-items:center;
  justify-content:center;height:100vh;gap:20px">
  <button id="start" style="padding:20px 40px;font-size:18px;
    cursor:pointer;background:#151520;color:#7ae;border:1px solid #7ae;
    border-radius:8px">
    Start Ambient Pad
  </button>
  <div id="info" style="opacity:0.5;font-size:14px"></div>
<script>
// Minimal 1D Perlin noise
class Perlin {
  constructor() {
    this.p = Array.from({length:512}, () => Math.floor(Math.random()*256));
  }
  fade(t) { return t*t*t*(t*(t*6-15)+10); }
  lerp(a,b,t) { return a+t*(b-a); }
  grad(h,x) { return (h&1)===0 ? x : -x; }
  noise(x) {
    const xi = Math.floor(x) & 255;
    const xf = x - Math.floor(x);
    return this.lerp(
      this.grad(this.p[xi], xf),
      this.grad(this.p[xi+1], xf-1),
      this.fade(xf)
    );
  }
}

document.getElementById('start').addEventListener('click', async () => {
  const ctx = new AudioContext();
  const perlin = new Perlin();

  // Reverb
  const conv = ctx.createConvolver();
  const irLen = ctx.sampleRate * 7;
  const ir = ctx.createBuffer(2, irLen, ctx.sampleRate);
  for (let ch = 0; ch < 2; ch++) {
    const d = ir.getChannelData(ch);
    for (let i = 0; i < irLen; i++) {
      d[i] = (Math.random()*2-1) * Math.pow(1-i/irLen, 2);
    }
  }
  conv.buffer = ir;

  const master = ctx.createGain();
  master.gain.value = 0.3;
  conv.connect(master);
  master.connect(ctx.destination);

  // Pad: 6 detuned oscillators per voice
  const voices = [];
  const baseFreqs = [110, 164.81, 220]; // A2, E3, A3

  baseFreqs.forEach(baseFreq => {
    const filter = ctx.createBiquadFilter();
    filter.type = 'lowpass';
    filter.frequency.value = 500;
    filter.Q.value = 2;
    filter.connect(conv);

    const detunes = [-12, -5, 0, 3, 7, 15];
    const oscs = detunes.map(d => {
      const osc = ctx.createOscillator();
      const g = ctx.createGain();
      osc.type = Math.random() > 0.5 ? 'sine' : 'triangle';
      osc.frequency.value = baseFreq;
      osc.detune.value = d;
      g.gain.value = 0;
      g.gain.linearRampToValueAtTime(0.04, ctx.currentTime + 6);
      osc.connect(g);
      g.connect(filter);
      osc.start();
      return osc;
    });

    voices.push({ filter, oscs });
  });

  // Perlin noise modulation loop
  const startTime = performance.now();
  const info = document.getElementById('info');

  function modulate() {
    const t = (performance.now() - startTime) / 1000;

    voices.forEach((voice, i) => {
      // Each voice gets a different Perlin "lane"
      const n = perlin.noise(t * 0.04 + i * 100);
      // Map [-1, 1] -> [150, 3000]
      const freq = 150 + (n + 1) * 0.5 * 2850;
      voice.filter.frequency.setValueAtTime(freq, ctx.currentTime);

      // Modulate Q
      const nQ = perlin.noise(t * 0.02 + i * 200 + 50);
      voice.filter.Q.setValueAtTime(1 + (nQ + 1) * 3, ctx.currentTime);
    });

    info.textContent = `Elapsed: ${Math.floor(t)}s`;
    requestAnimationFrame(modulate);
  }

  modulate();
  document.getElementById('start').textContent = 'Pad Active';
});
</script>
</body>
</html>
```

---

### Appendix: Key Resources

**Books and Theory**
- Susan Rogers & Ogi Ogas, *[This Is What It Sounds Like](https://www.thisiswhatitsoundslike.com/)* (2022) -- seven dimensions of music perception
- Brian Eno, liner notes to *Ambient 1: Music for Airports* (1978) -- foundational ambient manifesto

**Web Audio API Documentation**
- [MDN Web Audio API](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API)
- [MDN Spatialization Basics](https://developer.mozilla.org/en-US/docs/Web/API/Web_Audio_API/Web_audio_spatialization_basics)

**Libraries**
- [Tone.js](https://tonejs.github.io/) -- high-level Web Audio framework
- [Omnitone](https://github.com/GoogleChrome/omnitone) -- ambisonic spatial audio rendering
- [BinauralBeatJS](https://github.com/ichabodcole/BinauralBeatJS) -- binaural beat generation

**Generative Music Platforms**
- [Generative.fm](https://generative.fm/) -- endless browser-based ambient generators
- [Teropa - How Generative Music Works](https://teropa.info/loop/) -- interactive essay with playable examples
- [Brian Eno: Reflection](https://www.generativemusic.com/reflection.html) -- generative app

**Tutorials and Articles**
- [Reverb Machine - Deconstructing Music for Airports](https://reverbmachine.com/blog/deconstructing-brian-eno-music-for-airports/)
- [DEV Community - Drone Ambient Noise Synthesizer in JS](https://dev.to/max_alyokhin/drone-ambient-noise-synthesizer-in-javascript-when-instability-is-a-feature-not-a-bug-34i8)
- [DEV Community - Granular Synthesis in the Browser](https://dev.to/hexshift/granular-synthesis-in-the-browser-using-web-audio-api-and-audiobuffer-slicing-2o9h)
- [Alex Bainter - Making Generative Music in the Browser](https://medium.com/@alexbainter/making-generative-music-in-the-browser-bfb552a26b0b)
- [Algorithmic Composition Tutorial](https://junshern.github.io/algorithmic-music-tutorial/part1.html)

**Open Source Projects**
- [Chrome Music Lab](https://github.com/googlecreativelab/chrome-music-lab) -- interactive audio experiments
- [generative.fm source](https://github.com/generativefm/generative.fm) -- full platform source code
- [awesome-webaudio](https://github.com/notthetup/awesome-webaudio) -- curated list of Web Audio resources
- [pparocza/generative-music-web-audio](https://github.com/pparocza/generative-music-web-audio) -- synth architectures in JS
